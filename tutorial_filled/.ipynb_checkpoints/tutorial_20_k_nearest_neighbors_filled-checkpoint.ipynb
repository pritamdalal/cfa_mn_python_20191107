{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 20 - K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a supervised machine learning task, we attempt to predict a *label* from a set of *features*.  There are two categories of supervised problems; they are differentiated by the nature of the labels that are the target of the predicting.\n",
    "\n",
    "1. Regression - the label can take numeric values in a continuous range.\n",
    "2. Classification - the label can take a finite/discrete set of values.\n",
    "\n",
    "In the previous tutorial, we discussed *linear regression*, which is a simple approach to regression.  In this tutorial, we are going to introduce *K-nearest-neighbors* (KNN) which is a simple machine learning algorithm that can be used for both regression and classification. \n",
    "\n",
    "We are going to focus on KNN as a classifier.  In particular, we will set up a simple financial classification problem and use it as a way to explore various common themes in machine learning, including the variance-bias trade off and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading all the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import pandas as pd\n",
    "##> import numpy as np\n",
    "##> import seaborn as sns\n",
    "##> import sklearn\n",
    "##> import matplotlib.pyplot as plt\n",
    "##> %matplotlib inline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various VIX Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a brief background discussion of the data we will be analyzing.\n",
    "\n",
    "The VIX volatility index is published by the CBOE and is a measure of 30-day market implied volatility for the S&P 500 index.  Using that same methodology, the CBOE publishes other volatility measures on other stock indices and ETFs, such as the Russell 2000 and EWZ.  Most of the CBOE volatility measures have a 30-day tenor, meaning they are calculated using options that have (roughly) 30 days to maturity.\n",
    "\n",
    "There are, however, several CBOE volatility indices with different tenors.  For the S&P 500, in addition to the standard 30-day VIX, there are indices with the following tenors: 9-day, 3-month, 6-month, and 1-year.  The analysis in this tutorial is going to involve four of these different S&P 500 VIX tenors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading-In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read-in our data set into a variable called `df_vix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_vix = pd.read_csv('../data/vix_knn.csv')\n",
    "##> df_vix = df_vix[df_vix.trade_date > '2011-01-03'] #removing the first row of NaNs\n",
    "##> df_vix.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set consists of daily SPY returns, and also daily changes in the 9-day, 30-day, 3-month, 6-month VIX indices.  I excluded the 1-year index because there is limited historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Visualization Aside: Pair-Plots with Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping into classification with KNN, let's try the `pairplot()` function in the **seaborn** package. This function is useful for simulataneously visualizing pairwise relationships for several variables.\n",
    "\n",
    "Let's apply this function to the various VIX indices in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> sns.pairplot(df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge Question:** How would you characterize the pairwise relationship between the various VIX tenors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Simple Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen several times in this course, there is an inverse relationship between returns and implied volatility.  When returns are negative, implied vols increase; when returns are positive, implied vols decrease.  In the previous tutorial, we quantified this relationship with a `LinearRegression`, and found the relationship to be quite strong.\n",
    "\n",
    "\n",
    "Based our knowledge of this relationship let's consider a simple classifcation exercise: identify wheather a return was \"gain\" or \"loss\", based on changes in the various VIX indices.  \n",
    "\n",
    "In the language of machine learning:\n",
    "\n",
    "1. Label: today's return as a \"gain\" or \"loss\"\n",
    "2. Features: changes in the VIX indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Simple Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative purposes, and also for the purposes of identifying performance bounds, let's consider some simple classification schemes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A completely random predicition is right 50% of the time.  This represents the lower-bound of performance of any learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # setting random seed\n",
    "##> np.random.seed(100)\n",
    "##> \n",
    "##> # making random label predictions\n",
    "##> df_vix['rand_label'] = np.random.randint(0, 2, df_vix.shape[0])\n",
    "##> \n",
    "##> # masking conditions that identify successful predictions\n",
    "##> cond1 = (df_vix.rand_label == 0) & (df_vix.spy_ret <= 0)\n",
    "##> cond2 = (df_vix.rand_label == 1) & (df_vix.spy_ret >= 0)\n",
    "##> \n",
    "##> # using masking to calculate the success rate\n",
    "##> (df_vix[cond1 | cond2].shape[0]) / (df_vix.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### High Bias: Alway Guess 'Gain'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if always predict a gain?  This is a great example of a classifier with a high bias and low variance.  It's smarter than random guessing because it's rooted in the knowledge that over long stretches of time, equity markets tend to rise (and also that markets rarely ever jump upwards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # always predict gain\n",
    "##> (df_vix[df_vix.spy_ret > 0].shape[0]) / (df_vix.shape[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since about 55% of the days in `df_vix` were gains for SPY, this predictor would have been right 55% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leverage Effect Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also create a simple rule based classification that codifies our knowledge of the implied leverage effect.  The rule would simply be that if there is an increase in the VIX, predict a SPY loss; if there is a decrease in the VIX predict an SPY gain.\n",
    "\n",
    "This simply rule produces accurate labels 80% of the time, which further illustrates the strength of the implied leverate effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # conditions that define successful predictions\n",
    "##> cond1 = (df_vix.vix_030 >= 0) & (df_vix.spy_ret <= 0)\n",
    "##> cond2 = (df_vix.vix_030 <= 0) & (df_vix.spy_ret >= 0)\n",
    "##> \n",
    "##> # calculating the proportions of successful conditions\n",
    "##> (df_vix[cond1 | cond2].shape[0]) / (df_vix.shape[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated classifier involving VIX changes should perform at least as well as this, or else it is not adding much beyond simply capturing the leverage effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighboors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a simple classification algorithm that is based on this simple and intuitive principle: feature observations that are similiar, should have similar associated labels.  Feature similarity is determined by distance in Euclidean space.  \n",
    "\n",
    "\n",
    "Here is how the KNN works.  Suppose you are trying to predicate a label for a given feature observation:\n",
    "\n",
    "1. Find the $K$ feature samples in the the training set that are closest to your feature observation.\n",
    "\n",
    "1. Find the labels associated with those  $K$ closest samples.\n",
    "\n",
    "2. The KNN prediction is the label that occurs most often in that set of $K$ lables.\n",
    "\n",
    "KNN is an example of an *instance-based* classifier, there really is no \"fitting\" process other than storing the training data.  The prediction alogrithm amounts to calculating distances between the feature obervation and the feature in the train set, sorting the feature set by this distance, and then surveying the labels of the $K$ closest feature observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In anticipation of performing KNN on our data, let's add a label column and do some additional cleaning.\n",
    "\n",
    "The following code defines a simple function that will add a column of labels to our data set: `L` stands for a loss day, `G` stands for gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> def labeler(ret):\n",
    "##>     if ret < 0:\n",
    "##>         return('L')\n",
    "##>     else:\n",
    "##>         return('G')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's add a label column called `spy_label_0`.  We can do this conveniently with the `.apply()` method which has the effect of vectorizing our `labeler` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_vix.drop(['rand_label'], axis=1, inplace=True)\n",
    "##> df_vix['spy_label_0'] = df_vix['spy_ret'].apply(labeler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Current Day 'G' or 'L'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll apply KNN to various subsets of our VIX features, and train our model using the entirety of our data set.\n",
    "\n",
    "Let's begin by importing the contructor function that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.neighbors import KNeighborsClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit our classifier model, let's begin by predicting the current day label with the standard (30-day) VIX.  We are going to use a $K$ value of 10.  As usual, our steps are:\n",
    "\n",
    "1. Feature selection: identify the data we will be using to train with.\n",
    "1. Model selection: instantiate the model using the constructor, and set model hyperparameters.\n",
    "2. Fitting: using the `.fit()` of our instantiated mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # (1) feature selection \n",
    "##> X = df_vix[['vix_030']]\n",
    "##> y = df_vix['spy_label_0'].values\n",
    "##> \n",
    "##> # (2) model selection and hyper-parameters\n",
    "##> clf = KNeighborsClassifier(n_neighbors = 10)\n",
    "##> \n",
    "##> # (3) fitting the model\n",
    "##> clf.fit(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most fundamental metric for investigating the quality of a classifier is the `accuracy`, which is simply the proportion correct classifications.\n",
    "\n",
    "Recall that for a `LinearRegression`, the `.score()` method gave the $R^2$ of the model.  For a `KNeighborsClassifier` the `.score()` method gives the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> clf.score(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is much better than random guessing, or a constant guess of 'G'.  As compared to the simple leverage-effect rule, our KNN only slightly outperforms it in-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:**  Repeat the KNN above, but instead use a `n_neighbor = 1`.  What do you make of the accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add in the other VIX indices as additional features. Ultimately, we're interested in seeing if our in-sample accuracy improves.\n",
    "\n",
    "When considering multiple features it is important to normalize the features to make sure they are all the same order of magnitude.  We can do this easily with the `scale()` function, who's default behavior is to subtract the mean, and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # importing scale function\n",
    "##> from sklearn.preprocessing import scale\n",
    "##> \n",
    "##> # feature selection\n",
    "##> X = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\n",
    "##> y = df_vix['spy_label_0'].values\n",
    "##> \n",
    "##> # scaling fetures\n",
    "##> Xs = scale(X)\n",
    "##> \n",
    "##> # model selection\n",
    "##> clf = KNeighborsClassifier(n_neighbors = 10)\n",
    "##> \n",
    "##> # fitting the model\n",
    "##> clf.fit(X, y)\n",
    "##> \n",
    "##> # checking in-sample accuracy score\n",
    "##> print(clf.score(X, y))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a slight improvement in our in-sample accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** What would happen if we added option volume to our analysis, but didn't normalize our feature set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Sets: `train_test_split()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been only looking at our accuracy score in-sample, meaning we are simply calculating the accuracy score on the training set. This gives a skewed perception of how accurate the model will be on new data.\n",
    "\n",
    "In order to account for this, we can partition the data set into two subsets, one for training the data and one for testing the trained model.  The `model_selection` module contains a convenience function for splitting data into training sets and testing sets.  It is called `train_test_split()`.  Let's import it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again perform a KNN on the single 30-day VIX feature, but this time splitting the data set.  We will then calculate the accuracy score of the model on both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # (1) feature selection \n",
    "##> X = df_vix[['vix_030']]\n",
    "##> y = df_vix['spy_label_0'].values\n",
    "##> \n",
    "##> # scaling fetures\n",
    "##> Xs = scale(X)\n",
    "##> \n",
    "##> # train-test_split the data\n",
    "##> X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.20, random_state=0)\n",
    "##> \n",
    "##> # (2) model selection\n",
    "##> clf = KNeighborsClassifier(n_neighbors = 10)\n",
    "##> \n",
    "##> # (3) fitting the model\n",
    "##> clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fit our model, let's check for accuracy on both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print('Train Accuracy: ', np.round(clf.score(X_train, y_train), 4))\n",
    "##> print('Test Accuracy:  ', np.round(clf.score(X_test, y_test), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the accuracy on the test set is lower than the accuracy on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:**  Copy and paste the code from the example above, and then re-run it using a $K=1$.  Is the spread between training accuracy and testing accuracy larger or smaller than when $K=10$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we used `train_test_split()` to partition our data so that we could check the accuracy of our model on observations that were not used in the fitting process.  When doing this we, noticed that the accuracy of our model is lower on the testing set then on the training set.  The testing set accuracy is more reflective of how the model would perform in the wild. \n",
    "\n",
    "\n",
    "But why stop with just doing this splitting once?  We could could do a `train_test_split()` multiple times, each time producing a different test accuracy.  This collection of accuracies, in aggregate, would form a more robust mearsure of model performance.  This is precisely the notion of cross validation.\n",
    "\n",
    "The `cross_val_score()` function in the `model_selection` module provides a convenient way to perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # importing cross_val_score\n",
    "##> from sklearn.model_selection import cross_val_score\n",
    "##> \n",
    "##> # feature selection \n",
    "##> X = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\n",
    "##> y = df_vix['spy_label_0'].values\n",
    "##> \n",
    "##> # scaling fetures\n",
    "##> Xs = scale(X)\n",
    "##> \n",
    "##> \n",
    "##> # model selection\n",
    "##> clf = KNeighborsClassifier(n_neighbors = 10)\n",
    "##> \n",
    "##> \n",
    "##> # cross validation\n",
    "##> cross_val_score(clf, Xs, y, cv=5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above splits the data into five parts, and then performs a train-test-split on each of them.  Functions like these are one of the reasons that `sklearn` is such a popular library.  This fuction works the same with all kinds of different models and saves us from writing a lot of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Variance-Bias Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in a code challenge above, when $K=1$ the in-sample accuracy is perfect, but the out-of-sample accuracy is poor.  This is a classic illustration of over-fitting the data.  By setting $K=1$ we are allowing for maximum model complexity.  Said in another way, we are attributing a lot of informational value to each (noisy) training observation.\n",
    "\n",
    "The following code allows us to visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # feature selection \n",
    "##> X = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\n",
    "##> y = df_vix['spy_label_0'].values\n",
    "##> \n",
    "##> # scaling fetures\n",
    "##> Xs = scale(X)\n",
    "##> \n",
    "##> # train-test_split the data\n",
    "##> X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.20, random_state=0)\n",
    "##> \n",
    "##> # various choices of neighbors\n",
    "##> k_neighbors = list(range(1, 20))\n",
    "##> \n",
    "##> training_error = []\n",
    "##> testing_error = []\n",
    "##> \n",
    "##> for k in k_neighbors:\n",
    "##>     clf = KNeighborsClassifier(n_neighbors=k)\n",
    "##>     clf.fit(X_train, y_train)\n",
    "##>     training_error.append(clf.score(X_train, y_train))\n",
    "##>     testing_error.append(clf.score(X_test, y_test))\n",
    "##>     \n",
    "##> \n",
    "##> plt.figure(figsize=(8, 6))\n",
    "##> plt.plot(k_neighbors, training_error)\n",
    "##> plt.plot(k_neighbors, testing_error)\n",
    "##> plt.xlabel('Number of Neighbors K')\n",
    "##> plt.ylabel('Accuracy')\n",
    "##> plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we reduce the complexity of our algorithm (by increasing the value of $K$) the training accuracy and testing accuracy converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** Based on this graph, which $K$ value would you choose and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Selection with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we saw that an $n$-fold cross-validation will produce $n$ different test-set error, resulting from $n$ different training sets of the model.  Cross validation can be used for the purposes of hyper-parameter selection because averaging over the cross validation scores is a more robust measurment of model performance.  \n",
    "\n",
    "In the following code, we perform a 10-fold cross validaton for $K=1, \\ldots, 50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # feature selection \n",
    "##> X = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\n",
    "##> y = df_vix['spy_label_0'].values\n",
    "##> \n",
    "##> # scaling fetures\n",
    "##> Xs = scale(X)\n",
    "##> \n",
    "##> # creating odd list of K for KNN\n",
    "##> k_neighbors = list(range(1,50))\n",
    "##> \n",
    "##> # empty list that will hold cv scores\n",
    "##> cv_scores = []\n",
    "##> \n",
    "##> # perform 10-fold cross validation\n",
    "##> for k in k_neighbors:\n",
    "##>     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "##>     scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "##>     cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter selection is usually stated in terms of mis-classification rate, which is simply 1 - accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # changing to misclassification error\n",
    "##> MSE = [1 - x for x in cv_scores]\n",
    "##> \n",
    "##> # plot misclassification error vs k\n",
    "##> plt.figure(figsize=(8, 6))\n",
    "##> plt.plot(k_neighbors, MSE)\n",
    "##> plt.xlabel('Number of Neighbors K')\n",
    "##> plt.ylabel('Misclassification Error')\n",
    "##> plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** Based on this graph, which value of $K$ would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
