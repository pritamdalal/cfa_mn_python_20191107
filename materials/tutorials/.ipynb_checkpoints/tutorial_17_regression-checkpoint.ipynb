{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 17 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is an intuitive introduction to simple linear regression in a finance context.  In particular, we will fit two regressions that demonstrate the leverage effect in SPY.  Our focus will be on how to implement linear regression in Python, rather than on mathematical/statistical details.\n",
    "\n",
    "Linear Regression will also serve as a first introduction to `sklearn`, a popular package for implementing various machine learning tasks in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> %matplotlib inline\n",
    "##> import matplotlib.pyplot as plt\n",
    "##> import seaborn as sns; sns.set()\n",
    "##> import pandas as pd\n",
    "##> import numpy as np\n",
    "##> import sklearn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we will be analyzing in this tutorial is weekly metrics for SPY during 2014-2018.  Each row of the `DataFrame` is a set of observations from a specific week.  In particular:\n",
    "\n",
    "1. `realized_vol` - standard deviation of returns during period (annualized).\n",
    "2. `ret` - simple return for the period.\n",
    "3. `start_iv` - the implied vol (variance swap rate) at the start of the period.\n",
    "\n",
    "Let's read-in the data set and have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy = pd.read_csv('../data/spy_2014_2018_regression.csv')\n",
    "##> df_spy.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis: Implied Volatility as a Fear Index (Leverage Effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options are simple insurance contracts that are written on top of an underlying stock.  They protect against large moves in the price of the underlying.  Puts protect against downward moves, calls protect against upward moves.\n",
    "\n",
    "The *implied volatility* of a stock is a measurement that gauges how much market participants are willing to pay for options on that stock.  Thus, the implied volatility of a stock serves as a index of how fearful market participants are about large moves in the stock price.\n",
    "\n",
    "**The Leverage Effect:** For many stocks, especially index-ETFs, the following two relationships hold:\n",
    "\n",
    "1.  Implied volatility increases when the stock experiences losses (negative returns).\n",
    "\n",
    "2. Implied volatility decreases when the stock experiences gains (positive returns).  \n",
    "\n",
    "Let's try to see this relationship in our SPY weekly data by means of a simple scatter plot.\n",
    "\n",
    "First, let's create a new column in `df_spy` to capture the week over week change of the implied volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy['iv_change'] = (df_spy['start_iv'] - df_spy['start_iv'].shift(1)).shift(-1)\n",
    "##> df_spy.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot the weekly returns (`ret`) against the implied-vol changes (`iv_change`) using `pandas` built-in plotting functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy.plot.scatter('ret', 'iv_change', c='k', figsize=(6, 4));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is a negative relationship, which is what we would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Example 1:  Returns vs Change in Implied Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an exploratory data analysis situation, the visualization above may be all we would need to establish the existance of the leverage effect in SPY.  On the other hand, we may want to make this analysis more precise by fitting a *linear regression* line to the data.  A linear regression is a simple model that purports that `iv_change` is a linear function of the `ret`.  Intuitively, when fitting a linear regression we are trying to find the straight line that has the minimium aggregate distance from all the points in our data.\n",
    "\n",
    "In the language of statistics, the `ret` is the *independent* variable and the `iv_change` is the *dependent* variable.  The field of machine learning uses different terms: `ret` is called the *feature* and `iv_change` is called the *label*.  In a generic machine learning problem, we seek to predict a *label* from one or more *features*. \n",
    "\n",
    "We will use `sklearn` to fit a linear regression to our data.  The first step in any learning task with `sklearn` is to instantiate the model object with a constructor function, in this case `LinearRegression`.  We'll call our model variable `iv_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.linear_model import LinearRegression\n",
    "##> iv_model = LinearRegression(fit_intercept=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `fit_intercept=True` we are forcing the line to go through the origin.  This seems reasonable from visual inspection of the data.\n",
    "\n",
    "Next, we'll separate out the data that will be used to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_ret = df_spy[['ret']][0:-1] # features\n",
    "##> df_iv = df_spy[['iv_change']][0:-1] # labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit the model by using the `.fit()` method of `iv_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> iv_model.fit(df_ret, df_iv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check the intercept and coefficient of the line that was fit to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(iv_model.coef_)\n",
    "##> print(iv_model.intercept_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our linear regression model has deterimined that the best fitting line is of the form:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{iv_change} = -2.1747 \\cdot \\text{weekly_return}\n",
    "\\end{align*}\n",
    "\n",
    "This can be interpreted to mean that every 1% of positive weekly price return leads to a drop in implied volatility of about 2.175%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn` we can use the `.predict()` method of our fitted model `iv_model` to predict labels for a given set of features.  In our example, we can predict implied volatility changes for a given set of weekly returns.\n",
    "\n",
    "Let's try this for -5%, 0%, and 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> test_values = np.array([-0.05, 0, 0.01]).reshape(-1, 1)\n",
    "##> iv_model.predict(test_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `.predict()` method to graph our fitted line along with our data.  This is a bit of a hack, but it works great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> xfit = np.linspace(-0.08, 0.055, 100)         # range of line\n",
    "##> yfit = iv_model.predict(xfit[:, np.newaxis])  # model values in range\n",
    "##> \n",
    "##> df_spy.plot.scatter('ret', 'iv_change', c='k', figsize=(6, 4));\n",
    "##> plt.plot(xfit, yfit);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn`, all learning models have a `.score()` method which calculates some kind of measure of accuracy or fit.  For a `LinearRegression` model, `.score()` gives the $R^2$.\n",
    "\n",
    "The $R^{2}$ measures gives a sense for the goodness of fit of a linear regression.  It can be interpreted as the percent of variance in the label that is explained by the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> iv_model.score(df_ret, df_iv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our linear regression explains 59% of the variance in weekly implied volatility changes, from weekly returns.\n",
    "\n",
    "\n",
    "There is no universal notion of what is a good or bad $R^2$.  That type of value judgement is context specific.  Based on my experience of looking at financial data, this scatter plot looks pretty good, meaning that the relationship is strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Example 2: Realized Volatility Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stylized fact about financial asset returns is that realized volatility exhibits clustering.  This means that high volatility tends to be followed by high volatility, and low volatility tends to be followed by low volatility.\n",
    "\n",
    "Let's try to observe realized voaltility clustering in our weekly SPY data, and then analyze it with linear regression.  In particular, let's observe the relationship between current-week realized volaltility and subsequent-week realized volatility.\n",
    "\n",
    "We'll begin by first creating new columns in `df_spy` to hold this data.  Notice that `real_vol_0` is just a copy of `realized_vol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy['real_vol_0'] = df_spy['realized_vol']\n",
    "##> df_spy['real_vol_1'] = df_spy['realized_vol'].shift(-1)\n",
    "##> df_spy.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at a scatter plot of `real_vol_0` vs `real_vol_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy.plot.scatter('real_vol_0', 'real_vol_1', c='k', figsize=(6, 4));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, I would say this scatter plot looks good.  The relationship is clearly positive (although quite noisy), as we would expect from the stylized fact of volatility clustering.\n",
    "\n",
    "However, the data in data in it's current form is not particularly well suited for linear regression.  First of all, the volatilies are bunched near zero, with a few extremely large observations.  Additionally, standard deviations are by definition always greater than zero.\n",
    "\n",
    "\n",
    "For both of these reasons, let's take the logs of both variables to make the relationship more clear.  We'll do so by simply repopulating the columns in `df_spy` with the logged values that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy['real_vol_0'] = np.log(df_spy['realized_vol'])\n",
    "##> df_spy['real_vol_1'] = np.log(df_spy['realized_vol']).shift(-1)\n",
    "##> df_spy.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replot the $\\log$ data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy.plot.scatter('real_vol_0', 'real_vol_1', c='k', figsize=(6, 4));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive relatinoship looks more linear after taking logs of both the features and the labels, however it's still quite noisy.\n",
    "\n",
    "As in the previous section, let's fit a simple linear regression to this data by executing the following steps:\n",
    "\n",
    "1. Instantiate a model with the `LinearRegression()` constructor.\n",
    "\n",
    "1. Isolatethe data for fitting.\n",
    "\n",
    "2. Fit the model with `.fit()`.\n",
    "\n",
    "3. Check for goodness of fit with `.score()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's instantiate the `LinearRegression` model object, we'll call it `rv_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> rv_model = LinearRegression(fit_intercept=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's isolate the data that we will use to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_rv_0 = df_spy[['real_vol_0']][:-1]\n",
    "##> df_rv_1 = df_spy[['real_vol_1']][:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit the model to the data using the `.fit()` method of `rv_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> rv_model.fit(df_rv_0, df_rv_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can check the goodness of fit by first visually inspecting the data, and then also by calculating the $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> xfit = np.linspace(-4.5, -0.75, 100)          # range of line\n",
    "##> yfit = rv_model.predict(xfit[:, np.newaxis])  # model values in range\n",
    "##> \n",
    "##> df_spy.plot.scatter('real_vol_0', 'real_vol_1', c='k', figsize=(6, 4));\n",
    "##> plt.plot(xfit, yfit);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output code below, our $R^2$ is lower for this regression, than the previous one (`ret` vs `iv_change`).  This is rather obvious from visual inspection of the two graphs - notice how much more spread out the data points are in this graph, versus the graph in the previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> rv_model.score(df_rv_0, df_rv_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Realized Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, our regression analysis involved using the entirety of the five years of SPY data that we have avaialable.  This is typical if you are using regression for exploratory data analysis, or simply to confirm some kind of directional relationship between two variables.\n",
    "\n",
    "However, the aim of machine learning typically goes beyond mere exploration - the ultimate goal is usually prediction or forecasting.  If you're serious about that objective, it's appropriate to split your data into a *training* set and a *testing* set.  These separate sets are used for two distinct purposes, in a two-stage approach:\n",
    "\n",
    "1. Traing data - used to train/fit/learn the model.  This phase is referred to as the *learning* or *training* phase. \n",
    "\n",
    "2. Testing data  - fed into the trained model to produce predictions; we then analyze the predicted values vs true values in the test set, to determine the accuracy of the model.  This phase referred to as the *testing* or *generalization* phase.\n",
    "\n",
    "Let's try using this two-stage approach with our realized volatility data.  \n",
    "\n",
    "Specifically, rather than fitting a linear regression to the entirety of our data set, let's instead fit it to only the first four years of the data (2014-2017).  We'll then use the fitted/trained model to forecast realized volatility in 2018.\n",
    "\n",
    "Let's begin by instantiating a new `LinearRegression` object and call it `fcst_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> fcst_model = LinearRegression(fit_intercept=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's grab the training data from 2014-2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_rv_0_train = df_spy[['real_vol_0']][0:208] # this weeks volatility (feature)\n",
    "##> df_rv_1_train = df_spy[['real_vol_1']][0:208] # next weeks volatility (label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next fit our model to the training data using `fcst_model.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> fcst_model.fit(df_rv_0_train, df_rv_1_train) # fitting the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the coefficient, the intercept, and the $R^{2}$ from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(\"Coefficent:      \", np.round(fcst_model.coef_[0, 0], 2))    # coefficient\n",
    "##> print(\"Intercept:      \", np.round(fcst_model.intercept_[0], 2))  # intercept\n",
    "##> print(\"R^2 (training):  \", np.round(fcst_model.score(df_rv_0_train, df_rv_1_train), 2)) # R^2 from training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our linear model is: \n",
    "\n",
    "$$\\log(\\text{next_week_realized_vol}) = 0.46 * \\log(\\text{this_week_realized_vol}) - 1.34.$$\n",
    "\n",
    "It accounts for about 21% of the variability of the weekly $\\log(\\text{realized_vol})$ in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply our trained model to data from 2018.  We begin be separating out the 2018 *testing* data into it's own `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_rv_0_test = df_spy[['real_vol_0']][209:-1] # features\n",
    "##> df_rv_1_test = df_spy[['real_vol_1']][209:-1] # labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test how our model does against the real data by plugging our testing data into the score method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> fcst_model.score(df_rv_0_test, df_rv_1_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can first calculate the predictions, and then calculate the $R^2$ directly on the predicted values.  In order to do this we would use the `.predict()` method of the LinearRegression object along the `r2_score()` function in the `sklearn.metrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> sklearn.metrics.r2_score(df_rv_1_test, fcst_model.predict(df_rv_0_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model is more accurate (i.e. has a higher $R^2$) on the training set than on the testing set - this is always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating our Forecasting  Model Daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our forcasting exercise above, our `LinearRegression` model was trained on data from 2014-2017, and all of our 2018 forecasts were based on that model.  This is probably not what we would do in practice.  Instead, we would fit a new model on a regular basis.  \n",
    "\n",
    "In the following code, the training period is updated every week to the most recent four years.  We would hope to see slightly improved performance over just training the model once.  (The code below is also indcative of patterns used when conducting a backtest.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> ix_start = 0\n",
    "##> ix_end = 208\n",
    "##> \n",
    "##> forecasts = np.zeros(50)\n",
    "##> fcst_model_2 = LinearRegression(fit_intercept=True)\n",
    "##> for ix_end in range(208, 258, 1):\n",
    "##>     # setting training period start date   \n",
    "##>     ix_start = ix_end - 208\n",
    "##>     \n",
    "##>     # selecting training data\n",
    "##>     df_rv_0_train = df_spy[['real_vol_0']][ix_start:ix_end]\n",
    "##>     df_rv_1_train = df_spy[['real_vol_1']][ix_start:ix_end]\n",
    "##>     \n",
    "##>     # fitting the model\n",
    "##>     fcst_model_2.fit(df_rv_0_train, df_rv_1_train)\n",
    "##>     \n",
    "##>     # forecasting with the newly fitted model\n",
    "##>     real_vol = df_spy['real_vol_0'].values[ix_end]\n",
    "##>     fcst_rv = fcst_model_2.coef_[0, 0] * real_vol  + fcst_model_2.intercept_[0]\n",
    "##>     \n",
    "##>     # saving the current forecast\n",
    "##>     forecasts[ix_start] = fcst_rv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a very small improvement when updating the model daily - an $R^2$ of 0.1817 vs 0.1484.  I would not expect the improvement to be that significant given the simplistic nature of our forecasting mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> sklearn.metrics.r2_score(df_spy[['real_vol_1']][208:258], forecasts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "*PDSH* 5.1 - What Is Machine Learning?\n",
    "\n",
    "*PDSH* 5.2 - Introducing Scikit-Learn\n",
    "\n",
    "*PDSH* 5.3 - Hyperparameters and Model Validation\n",
    "\n",
    "*PDSH* 5.4 - Feature Engineering\n",
    "\n",
    "*PDSH* 5.6 - In Depth: Linear Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
